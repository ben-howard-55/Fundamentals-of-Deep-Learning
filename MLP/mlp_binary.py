# -*- coding: utf-8 -*-
"""MLP-Binary.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mwFmSWvClYWPNKH_IFz7HXj-QCkjVH7v

# Binary Multilayer Perceptron
## Using Ionosphere dataset
Multilayer Perceptrons or MLPs are fully connected Neural Networks, where each hidden layer is dense.

Usefule for:

1. Binary Classification
2. Multiclass Classification
3. Regression

# MLP for Binary Classification.
- Using a dataset that is used to determine whether a signal originates from or outside our atmosphere. (Ionosphere Dataset).

Imports for data processing:
"""

# pre imports
from pandas import read_csv
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense

"""Loading the dataset:"""

path = "https://raw.githubusercontent.com/jbrownlee/Datasets/master/ionosphere.csv"

df = read_csv(path, header=None)
print(df)

"""Splitting the data into output and input"""

X, Y = df.values[:, :-1], df.values[:, -1]

X = X.astype('float32')
print(X)

"""Encoding string outputs as integers"""

Y = LabelEncoder().fit_transform(Y)

"""Splitting the input and output into test and training samples."""

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33)

"""There are 116 tests and 235 training data points. Each datapoint contains 34 features."""

print(X_test.shape, X_train.shape, y_test.shape, y_train.shape)

Get Number of input features

n_features = X_train.shape[1]

"""# 1. Defining the Model

The code below produces 2 Hidden layers, an input and output layer.
- The input layer is of shape 34.
- The first hidden layer has 10 ReLU neurons and is initilaized with he_normal weights. (truncated normal dist.)
- The second hidden layer has 8 ReLU neruons with the same initializer as above.
- The output layer has 1 neuron due to the binary nature of the problem.
"""

model = Sequential()
model.add(
    Dense(10, 
          activation='relu', 
          kernel_initializer='he_normal', 
          input_shape=(n_features,)
          )
    )
model.add(
    Dense(8,
          activation="relu",
          kernel_initializer='he_normal')
  )
model.add(Dense(1, activation="sigmoid"))

"""## 2. Compiling the Model
Here Compilation is used using the 'adam' optimizer model.
Loss is calculated by crossentropy binary loss and the metric used for analysis is accuracy.
"""

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

"""# 3. Fitting the Model
The fitting of the model will be occuring with the training data set (66%).
- 150 Epochs will be used.
- The Batch Size is 32.
- Verbose is set to 0, so no outputs.
"""

model.fit(X_train, y_train, epochs=150, batch_size=32, verbose=0)

"""# 4. Evaluating the Model
Running the trained model with the test data produces two variables:
1. Loss (cross entropy loss)
2. Accuracy
*In this case accuracy was 92.2%*
"""

loss, acc = model.evaluate(X_test, y_test, verbose=0)
print('Test Accuraxcy: %.3f' % acc)

"""# 5. Make a Prediction
Using data without an output, make a prediction.
*This predicted 0.991 for input, meaning most likely in class 1.
"""

row = [1,0,0.99539,-0.05889,0.85243,0.02306,0.83398,-0.37708,1,0.03760,0.85243,-0.17755,0.59755,-0.44945,0.60536,-0.38223,0.84356,-0.38542,0.58212,-0.32192,0.56971,-0.29674,0.36946,-0.47357,0.56811,-0.51171,0.41078,-0.46168,0.21266,-0.34090,0.42267,-0.54487,0.18641,-0.45300]
y_hat = model.predict([row])
print('Predicted: %.3f' % y_hat)